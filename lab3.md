
**Группа: ПИН-б-о-22-1**
**Студент: Подгайный Александр**

# Цель работы
Освоить базовые принципы работы с низкоуровневой абстракцией Apache Spark — RDD (Resilient Distributed Dataset). Получить практические навыки создания RDD, применения основных трансформаций и действий, а также построения и выполнения последовательностей операций (DAG).

# Ход работы
1. Подготовка окружения
Установлены и настроены:

Apache Spark 3.5.0

Python 3.10

PySpark

Jupyter Notebook (для интерактивной разработки)

Создан тестовый файл log.txt с логами веб-сервера в формате Common Log Format.

2. Часть 1: Базовые операции с RDD
Задачи:

Создать RDD из коллекции чисел и текстового файла.

Применить трансформации filter() и map().

Выполнить действия count() и take().

python
# Создание RDD из коллекции
numbers_rdd = sc.parallelize(range(1, 1001))

# Фильтрация чётных чисел и возведение в квадрат
even_squared = numbers_rdd.filter(lambda x: x % 2 == 0).map(lambda x: x ** 2)

# Действия
print(even_squared.count())
print(even_squared.take(10))
3. Часть 2: Анализ логов веб-сервера
Задача: Найти 10 самых часто запрашиваемых URL.

Алгоритм:

Загрузка логов → sc.textFile()

Извлечение URL → пользовательская функция extract_url()

Преобразование в пары (url, 1) → map()

Агрегация → reduceByKey()

Сортировка → sortBy(..., ascending=False)

Вывод топ-10 → take(10)

Ключевой код:

python
def extract_url(line):
    try:
        parts = line.split()
        return parts[6] if len(parts) > 6 else ''
    except:
        return ''

url_counts = log_rdd.map(extract_url) \
                    .filter(lambda x: x) \
                    .map(lambda url: (url, 1)) \
                    .reduceByKey(lambda a, b: a + b) \
                    .sortBy(lambda x: x[1], ascending=False)

top10 = url_counts.take(10)
# Результаты выполнения
Часть 1: Базовые операции
text
Чётные числа (первые 10): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
Чётные числа в квадрате (первые 10): [4, 16, 36, 64, 100, 144, 196, 256, 324, 400]
Количество чётных чисел: 500
Количество строк с ERROR: 2
Часть 2: Анализ логов
text
Топ-10 самых часто запрашиваемых URL:
1. /index.html       3 запроса
2. /contact.html     1 запрос
3. /about.html       1 запрос
4. /login            1 запрос
5. /error            1 запрос
6. /notfound         1 запрос

# Ответы на контрольные вопросы
1. Что такое RDD и каковы его основные свойства?
RDD — это распределённая, отказоустойчивая, неизменяемая коллекция объектов в Spark.
Свойства:

Resilient: Восстанавливается после сбоев через lineage.

Distributed: Разделено на партиции, обрабатывается параллельно.

Immutable: Неизменяемо после создания.

Lazy Evaluated: Вычисления происходят только при вызове действий.

2. Чем отличаются трансформации от действий?
Трансформации создают новое RDD из существующего (например, map, filter). Они «ленивые» и не выполняются сразу.

Действия запускают вычисления и возвращают результат в драйвер (например, count, collect). Они «заставляют» Spark выполнить все трансформации в цепочке.

3. Что такое lineage в Spark?
Lineage — это логическая цепочка операций (граф зависимостей), которая описывает, как RDD было получено из других RDD. Используется для восстановления данных при сбоях.

4. Как происходит восстановление данных при сбоях?
При потере данных Spark использует lineage, чтобы пересчитать потерянные партиции, выполняя цепочку трансформаций заново на основе исходных данных или предыдущих RDD.

